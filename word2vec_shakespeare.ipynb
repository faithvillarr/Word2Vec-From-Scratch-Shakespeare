{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec: Shakespeare\n",
    "Word2Vec is a genius method of converting the semntic value and associations of words into a lower dimensional vector space. \n",
    "<br> <br>\n",
    "In this notebook, we will be taking the many words of shakespeare and using them to train a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = open(\"alllines.txt\", 'r').read()\n",
    "\n",
    "text = text.lower()\n",
    "text= text[: len(text) // 2]\n",
    "# Extract tokens\n",
    "tokens = re.split(r'\\W+', text)\n",
    "tokens = [token for token in re.split(r'\\W+', text) if token]\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "    \n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "# word_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Data (kinda)\n",
    "\n",
    "Due to the size of the dataset and the huge vectors required (for 22,602 words total ;-;), we are writing to a csv file to store the word pairs created by our window size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the relevant id for a given word\n",
    "def get_id(word):\n",
    "    return word_to_id[word]\n",
    "\n",
    "# Define concat function for indices\n",
    "def concat(*iterables):\n",
    "    for iterable in iterables:\n",
    "        yield from iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the CSV file name\n",
    "csv_file = 'word_pairs.csv'\n",
    "\n",
    "def generate_word_pairs(tokens, window):\n",
    "    word_pairs = []\n",
    "    n_tokens = len(tokens)\n",
    "    \n",
    "    for i in range(n_tokens):\n",
    "        indices = concat(\n",
    "            range(max(0, i - window), i), \n",
    "            range(i, min(n_tokens, i + window + 1))\n",
    "        )\n",
    "        for j in indices:\n",
    "            if i == j:\n",
    "                continue\n",
    "            first_word = word_to_id[tokens[i]]\n",
    "            second_word = word_to_id[tokens[j]]\n",
    "            word_pairs.append((first_word, second_word))\n",
    "    \n",
    "    return word_pairs\n",
    "\n",
    "pairs = generate_word_pairs(tokens, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pairs[0]\n",
    "# id_to_word[11115]\n",
    "# id_to_word[434]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_vec_from_id(ind):\n",
    "    res_vec = torch.zeros(len(word_to_id), dtype=torch.float32)  \n",
    "    res_vec[ind] = 1  \n",
    "    return res_vec\n",
    "\n",
    "\n",
    "def generate_training_data(pair): # Feed a pair, NOT pairs\n",
    "    train = pair[0]\n",
    "    label = pair[1]\n",
    "\n",
    "    train_vec = get_vec_from_id(train)\n",
    "    label_vec = get_vec_from_id(label)\n",
    "\n",
    "    return [train_vec, label_vec]\n",
    "\n",
    "# Testing a batch of 32 can load\n",
    "# temp = []\n",
    "# for i in range(0, 32):\n",
    "#     temp.append(generate_training_data(pairs[i]))\n",
    "    # print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|â–Ž         | 46560/1669986 [00:23<13:24, 2018.24 samples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Zero gradients, backward pass, and optimization\u001b[39;00m\n\u001b[0;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 60\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Update the progress bar\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_size)  # Input layer to hidden layer\n",
    "        self.activation = nn.ReLU()                      # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)   # Hidden layer to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)                # Linear transformation to hidden layer\n",
    "        x = self.activation(x)         # Activation function\n",
    "        x = self.fc2(x)                # Linear transformation to output layer\n",
    "        return x                       # Logits for output\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(word_to_id)  # Number of words in your vocabulary\n",
    "pair_size = len(pairs)\n",
    "hidden_size = 100              # Size of hidden layer\n",
    "learning_rate = 0.01           # Learning rate\n",
    "num_epochs = 10                # Number of epochs\n",
    "batch_size = 32                # Try 32 batch size \n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleNN(vocab_size, hidden_size)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    # Wrap the training loop with tqdm to show the progress bar\n",
    "    with tqdm(total=len(pairs), desc=f'Epoch {epoch + 1}/{num_epochs}', unit=' samples') as pbar:\n",
    "        for start_index in range(0, len(pairs), batch_size):\n",
    "            end_index = start_index + batch_size\n",
    "            batch_pairs = pairs[start_index:end_index]\n",
    "\n",
    "            # Initialize tensors for center and context\n",
    "            center_batch = torch.zeros(batch_size, vocab_size)  # Shape: (batch_size, vocab_size)\n",
    "            context_batch = torch.zeros(batch_size, vocab_size)  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "            for i, pair in enumerate(batch_pairs):\n",
    "                center_vec, context_vec = generate_training_data(pair)\n",
    "                center_batch[i] = center_vec.clone().detach()  # Assign the center vector to the batch\n",
    "                context_batch[i] = context_vec.clone().detach()  # Assign the context vector to the batch\n",
    "\n",
    "            # Forward pass\n",
    "            log_probs = model(center_batch)  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_function(log_probs, context_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Zero gradients, backward pass, and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(batch_size)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(pairs):.4f}')\n",
    "\n",
    "# Save the model and optimizer state\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Assume the model has already been trained\n",
    "# Extract weights (embeddings) from the first fully connected layer\n",
    "embeddings = model.fc1.weight.data.cpu().numpy()  # Shape: (hidden_size, vocab_size)\n",
    "\n",
    "# Reduce dimensions using PCA or t-SNE\n",
    "def visualize_embeddings(embeddings, words, method='pca', n_components=2):\n",
    "    if method == 'pca':\n",
    "        pca = PCA(n_components=n_components)\n",
    "        reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    elif method == 'tsne':\n",
    "        tsne = TSNE(n_components=n_components)\n",
    "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    sns.scatterplot(x=reduced_embeddings[:, 0], y=reduced_embeddings[:, 1])\n",
    "\n",
    "    # plt.xlim(-5, 5)\n",
    "    # plt.ylim(-5, 5)\n",
    "\n",
    "    # Annotate points with words from the vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]))\n",
    "\n",
    "    plt.title(f\"Word Embeddings Visualization using {method.upper()}\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the embeddings\n",
    "word_list = list(word_to_id.keys())  # Assuming word_to_id is a dictionary mapping words to indices\n",
    "visualize_embeddings(embeddings, word_list)  # You can switch 'tsne' to 'pca'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
